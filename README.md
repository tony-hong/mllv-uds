# Multi-Modal Learning Seminar

Welcome to the multi-modal learning discussion! This discussion will focus on papers related to multi-modal learning, specifically on language and vision.

Lecturer:  Xudong (Tony) Hong, Ruitao Feng


## Papers

The following papers will be discussed:

- Hu, R. and Singh, A., 2021. [Unit: Multimodal multitask learning with a unified transformer](https://openaccess.thecvf.com/content/ICCV2021/html/Hu_UniT_Multimodal_Multitask_Learning_With_a_Unified_Transformer_ICCV_2021_paper.html?ref=https://githubhelp.com). In Proceedings of the IEEE/CVF International Conference on Computer Vision (pp. 1439-1449).
- Lu, J., Batra, D., Parikh, D. and Lee, S., 2019. [Vilbert: Pretraining task-agnostic visiolinguistic representations for vision-and-language tasks](https://proceedings.neurips.cc/paper/2019/hash/c74d97b01eae257e44aa9d5bade97baf-Abstract.html). Advances in neural information processing systems, 32.
- Radford, A., Kim, J.W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J. and Krueger, G., 2021, July. [Learning transferable visual models from natural language supervision](http://proceedings.mlr.press/v139/radford21a). In International conference on machine learning (pp. 8748-8763). PMLR.

- Wang, B., Ma, L., Zhang, W., Jiang, W. and Zhang, F., 2019, July. [Hierarchical photo-scene encoder for album storytelling](https://ojs.aaai.org/index.php/AAAI/article/view/4918). In Proceedings of the AAAI Conference on Artificial Intelligence (Vol. 33, No. 01, pp. 8909-8916).
- Wang, R., Wei, Z., Cheng, Y., Li, P., Shan, H., Zhang, J., Zhang, Q. and Huang, X., 2019. [Keep it consistent: Topic-aware storytelling from an image stream via iterative multi-agent communication](https://arxiv.org/abs/1911.04192). arXiv preprint arXiv:1911.04192.
- Yu, Y., Chung, J., Yun, H., Kim, J. and Kim, G., 2021. [Transitional adaptation of pretrained models for visual storytelling](https://openaccess.thecvf.com/content/CVPR2021/html/Yu_Transitional_Adaptation_of_Pretrained_Models_for_Visual_Storytelling_CVPR_2021_paper.html). In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (pp. 12658-12668).
- Hu, J., Cheng, Y., Gan, Z., Liu, J., Gao, J. and Neubig, G., 2020, April. [What makes a good story? designing composite rewards for visual storytelling](https://ojs.aaai.org/index.php/AAAI/article/view/6305). In Proceedings of the AAAI Conference on Artificial Intelligence (Vol. 34, No. 05, pp. 7969-7976).
- Li, T. and Li, S., 2019. [Incorporating textual evidence in visual storytelling](https://arxiv.org/abs/1911.09334). arXiv preprint arXiv:1911.09334.

- Tsimpoukelli, M., Menick, J.L., Cabi, S., Eslami, S.M., Vinyals, O. and Hill, F., 2021. [Multimodal few-shot learning with frozen language models](https://proceedings.neurips.cc/paper/2021/hash/01b7575c38dac42f3cfb7d500438b875-Abstract.html). Advances in Neural Information Processing Systems, 34, pp.200-212.
- Zhang, Z., Guo, W., Meng, X., Wang, Y., Wang, Y., Jiang, X., Liu, Q. and Yang, Z., 2022. [Hyperpelt: Unified parameter-efficient language model tuning for both language and vision-and-language tasks](https://arxiv.org/abs/2203.03878). arXiv preprint arXiv:2203.03878.
- Sung, Y.L., Cho, J. and Bansal, M., 2022. [Vl-adapter: Parameter-efficient transfer learning for vision-and-language tasks](https://openaccess.thecvf.com/content/CVPR2022/html/Sung_VL-Adapter_Parameter-Efficient_Transfer_Learning_for_Vision-and-Language_Tasks_CVPR_2022_paper.html). In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (pp. 5227-5237).

- Zhang, Z., Zhang, A., Li, M., Zhao, H., Karypis, G. and Smola, A., 2023. [Multimodal chain-of-thought reasoning in language models](https://arxiv.org/abs/2302.00923). arXiv preprint arXiv:2302.00923.

- Ruan, L., Ma, Y., Yang, H., He, H., Liu, B., Fu, J., Yuan, N.J., Jin, Q. and Guo, B., 2022. [MM-Diffusion: Learning Multi-Modal Diffusion Models for Joint Audio and Video Generation](https://arxiv.org/abs/2212.09478). arXiv preprint arXiv:2212.09478.


## Discussion Format

We will have a group discussion on each paper, where participants can share their thoughts and insights on the research. The discussion will be led by a moderator who will facilitate the conversation and keep the discussion on topic.


## Date and Time

The date and time for the discussion will be determined soon. Please stay tuned for updates.


## Contact

If you have any questions or concerns, please contact us at [email address]. We look forward to seeing you at the discussion!


# Seminar: Machine Learning for Language and Vision

In this seminar, we will be discussing several recent papers on machine learning for natural language processing and computer vision applications. This seminar is perfect for students who are interested in working with artificial intelligence systems that deal with text and image data. 

Lecturer:  Xudong (Tony) Hong, Ruitao Feng

## (The following is under construction. Please stay tuned.)

## Introduction

We are pleased to invite you to participate in our upcoming discussion seminar on multi-modal learning, with a focus on language and vision. In this seminar, we will be discussing several recent papers related to this topic, with the aim of fostering a deeper understanding of the challenges and opportunities presented by multi-modal learning.

Multi-modal learning is a rapidly growing field that seeks to develop algorithms and models that can process and integrate information from multiple sources, such as text, images, and speech. In recent years, there has been a surge of interest in multi-modal learning, driven by the increasing availability of large-scale datasets and powerful computational resources.

One of the most exciting areas of research within multi-modal learning is the integration of language and vision. By combining natural language processing and computer vision techniques, researchers have developed models that can perform a wide range of tasks, such as image captioning, visual question answering, and visual storytelling.

During the seminar, we will be discussing several recent papers that explore different aspects of multi-modal learning with a focus on language and vision. We hope that this seminar will provide a forum for lively and insightful discussions, and that it will contribute to the development of new ideas and approaches for solving some of the most pressing challenges in this field.


## Papers

The following papers will be discussed:

- Hu, R. and Singh, A., 2021. [Unit: Multimodal multitask learning with a unified transformer](https://openaccess.thecvf.com/content/ICCV2021/html/Hu_UniT_Multimodal_Multitask_Learning_With_a_Unified_Transformer_ICCV_2021_paper.html?ref=https://githubhelp.com). In Proceedings of the IEEE/CVF International Conference on Computer Vision (pp. 1439-1449).
- Lu, J., Batra, D., Parikh, D. and Lee, S., 2019. [Vilbert: Pretraining task-agnostic visiolinguistic representations for vision-and-language tasks](https://proceedings.neurips.cc/paper/2019/hash/c74d97b01eae257e44aa9d5bade97baf-Abstract.html). Advances in neural information processing systems, 32.
- Radford, A., Kim, J.W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J. and Krueger, G., 2021, July. [Learning transferable visual models from natural language supervision](http://proceedings.mlr.press/v139/radford21a). In International conference on machine learning (pp. 8748-8763). PMLR.

- Wang, B., Ma, L., Zhang, W., Jiang, W. and Zhang, F., 2019, July. [Hierarchical photo-scene encoder for album storytelling](https://ojs.aaai.org/index.php/AAAI/article/view/4918). In Proceedings of the AAAI Conference on Artificial Intelligence (Vol. 33, No. 01, pp. 8909-8916).
- Wang, R., Wei, Z., Cheng, Y., Li, P., Shan, H., Zhang, J., Zhang, Q. and Huang, X., 2019. [Keep it consistent: Topic-aware storytelling from an image stream via iterative multi-agent communication](https://arxiv.org/abs/1911.04192). arXiv preprint arXiv:1911.04192.
- Yu, Y., Chung, J., Yun, H., Kim, J. and Kim, G., 2021. [Transitional adaptation of pretrained models for visual storytelling](https://openaccess.thecvf.com/content/CVPR2021/html/Yu_Transitional_Adaptation_of_Pretrained_Models_for_Visual_Storytelling_CVPR_2021_paper.html). In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (pp. 12658-12668).
- Hu, J., Cheng, Y., Gan, Z., Liu, J., Gao, J. and Neubig, G., 2020, April. [What makes a good story? designing composite rewards for visual storytelling](https://ojs.aaai.org/index.php/AAAI/article/view/6305). In Proceedings of the AAAI Conference on Artificial Intelligence (Vol. 34, No. 05, pp. 7969-7976).
- Li, T. and Li, S., 2019. [Incorporating textual evidence in visual storytelling](https://arxiv.org/abs/1911.09334). arXiv preprint arXiv:1911.09334.

- Tsimpoukelli, M., Menick, J.L., Cabi, S., Eslami, S.M., Vinyals, O. and Hill, F., 2021. [Multimodal few-shot learning with frozen language models](https://proceedings.neurips.cc/paper/2021/hash/01b7575c38dac42f3cfb7d500438b875-Abstract.html). Advances in Neural Information Processing Systems, 34, pp.200-212.
- Zhang, Z., Guo, W., Meng, X., Wang, Y., Wang, Y., Jiang, X., Liu, Q. and Yang, Z., 2022. [Hyperpelt: Unified parameter-efficient language model tuning for both language and vision-and-language tasks](https://arxiv.org/abs/2203.03878). arXiv preprint arXiv:2203.03878.
- Sung, Y.L., Cho, J. and Bansal, M., 2022. [Vl-adapter: Parameter-efficient transfer learning for vision-and-language tasks](https://openaccess.thecvf.com/content/CVPR2022/html/Sung_VL-Adapter_Parameter-Efficient_Transfer_Learning_for_Vision-and-Language_Tasks_CVPR_2022_paper.html). In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (pp. 5227-5237).

- Zhang, Z., Zhang, A., Li, M., Zhao, H., Karypis, G. and Smola, A., 2023. [Multimodal chain-of-thought reasoning in language models](https://arxiv.org/abs/2302.00923). arXiv preprint arXiv:2302.00923.

- Ruan, L., Ma, Y., Yang, H., He, H., Liu, B., Fu, J., Yuan, N.J., Jin, Q. and Guo, B., 2022. [MM-Diffusion: Learning Multi-Modal Diffusion Models for Joint Audio and Video Generation](https://arxiv.org/abs/2212.09478). arXiv preprint arXiv:2212.09478.


## Discussion Format

We will have a group discussion on each paper, where participants can share their thoughts and insights on the research. The discussion will be led by a moderator who will facilitate the conversation and keep the discussion on topic.


## Date and Time

The date and time for the discussion will be determined soon. Please stay tuned for updates.


## Contact

If you have any questions or concerns, please contact us at [email address]. We look forward to seeing you at the discussion!

